{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72Xytg-LGjIK"
      },
      "source": [
        "## PART 1\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5EPubBqIFIN"
      },
      "source": [
        "### Explanation of Python Packages for RAG Model\n",
        "\n",
        "This setup includes three essential Python packages: `pinecone-client`, `cohere`, and `transformers`. Here’s a brief overview of each library’s functions and applications:\n",
        "\n",
        "1. **`pinecone-client`**:\n",
        "   - **Objective**: Serves as the official client for Pinecone, a vector database designed for efficient storage, organization, and retrieval of vectors, particularly for similarity searches.\n",
        "   - **Application**: Ideal for managing large datasets or documents by evaluating vectorized text representations, enabling fast similarity searches to access relevant information.\n",
        "\n",
        "2. **`cohere`**:\n",
        "   - **Objective**: Provides powerful natural language processing (NLP) APIs for text generation and embeddings. This package allows the creation of embeddings for both documents and queries, as well as generating text from prompts.\n",
        "   - **Application**: Frequently used in QA systems and chatbots to produce searchable text or embeddings in vector databases like Pinecone.\n",
        "\n",
        "3. **`transformers`**:\n",
        "   - **Objective**: Developed by Hugging Face, this library simplifies the use of advanced deep learning models, primarily transformer-based (e.g., GPT, BERT). It supports a variety of NLP tasks, including text classification, translation, and summarization.\n",
        "   - **Application**: Useful for generating embeddings or providing direct responses in QA systems by integrating transformer models into the workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXKFLt-hBIae",
        "outputId": "4fe48117-3cf7-4317-83d2-104a0358e46d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting cohere\n",
            "  Downloading cohere-5.9.2-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.8.30)\n",
            "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Collecting boto3<2.0.0,>=1.34.0 (from cohere)\n",
            "  Downloading boto3-1.35.19-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx>=0.21.2 (from cohere)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.9.1)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.23.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.19.1)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20240914-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Collecting botocore<1.36.0,>=1.35.19 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading botocore-1.35.19-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx>=0.21.2->cohere)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.21.2->cohere)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.19->boto3<2.0.0,>=1.34.0->cohere) (2.8.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.19->boto3<2.0.0,>=1.34.0->cohere) (1.16.0)\n",
            "Downloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-5.9.2-py3-none-any.whl (222 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.4/222.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading boto3-1.35.19-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading types_requests-2.32.0.20240914-py3-none-any.whl (15 kB)\n",
            "Downloading botocore-1.35.19-py3-none-any.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: types-requests, pinecone-plugin-interface, parameterized, jmespath, httpx-sse, h11, fastavro, pinecone-plugin-inference, httpcore, botocore, s3transfer, pinecone-client, httpx, boto3, cohere\n",
            "Successfully installed boto3-1.35.19 botocore-1.35.19 cohere-5.9.2 fastavro-1.9.7 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 httpx-sse-0.4.0 jmespath-1.0.1 parameterized-0.9.0 pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7 s3transfer-0.10.2 types-requests-2.32.0.20240914\n"
          ]
        }
      ],
      "source": [
        "!pip install pinecone-client cohere transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edu33wrxBpr6",
        "outputId": "82b6b661-f653-474d-bd7a-34754c2b28d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pinecone\n",
            "  Downloading pinecone-5.1.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2024.8.30)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from pinecone) (1.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.0.7)\n",
            "Downloading pinecone-5.1.0-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.5/245.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pinecone\n",
            "Successfully installed pinecone-5.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pinecone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL6KAZ13G2iU"
      },
      "source": [
        "1. **`import Pinecone from pinecone`**:  \n",
        "   - Imports the `Pinecone` class to interact with the Pinecone vector database.\n",
        "\n",
        "2. **`pc = Pinecone(api_key=\"b10a42ec-9e36-4523-a2e1-08ce6de826f6\")`**:  \n",
        "   - Creates a Pinecone client instance using the specified API key for managing indexes.\n",
        "\n",
        "3. **`pc.Index(\"index384\")`**:  \n",
        "   - Accesses or creates a Pinecone index named `\"index384\"`, enabling insertion, retrieval, and similarity search of vector embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MrJDZwNUBtOY"
      },
      "outputs": [],
      "source": [
        "import pinecone\n",
        "\n",
        "# Initialize Pinecone client\n",
        "pinecone.init(api_key=\"b10a42ec-9e36-4523-a2e1-08ce6de826f6\")\n",
        "\n",
        "# Access the index\n",
        "index = pinecone.Index(\"index384\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq0Cd6VFHIn9"
      },
      "source": [
        "1. **`import cohere`**:  \n",
        "   - Imports the `cohere` library to utilize the Cohere API for tasks like text generation and embeddings.\n",
        "\n",
        "2. **`from transformers import AutoTokenizer, AutoModel`**:  \n",
        "   - Imports `AutoTokenizer` and `AutoModel` from the `transformers` library to handle text tokenization and access pre-trained models (e.g., BERT, GPT).\n",
        "\n",
        "3. **`import torch`**:  \n",
        "   - Imports PyTorch for tensor manipulation and building machine learning models.\n",
        "\n",
        "4. **`cohere_api_key = \"LgaWUuuPnuamPELt1VTEqP6WmwEYOjfLKFrsUg6P\"`**:  \n",
        "   - Defines the API key needed for authenticating requests to the Cohere service.\n",
        "\n",
        "5. **`co = cohere.Client(cohere_api_key)`**:  \n",
        "   - Initializes a Cohere client using the provided API key to access text generation and embedding services.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "09t087p-BYFI"
      },
      "outputs": [],
      "source": [
        "import cohere\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Initialize Cohere client with the API key\n",
        "client = cohere.Client(\"LgaWUuuPnuamPELt1VTEqP6WmwEYOjfLKFrsUg6P\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtIDSj0fKYQ4"
      },
      "source": [
        "### Loading the Tokenizer and Model: `sentence-transformers/all-MiniLM-L6-v2`\n",
        "\n",
        "1. **`tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')`**:  \n",
        "   - Initializes a tokenizer for the `all-MiniLM-L6-v2` model, converting text into token IDs.\n",
        "\n",
        "2. **`model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')`**:  \n",
        "   - Loads the pre-trained `all-MiniLM-L6-v2` model to create text embeddings.\n",
        "\n",
        "3. **`def embed_text(text):`**:  \n",
        "   - Defines a function `embed_text` to generate embeddings from input text.\n",
        "\n",
        "4. **Tokenizer Processing**:  \n",
        "   - The text is tokenized with padding and truncation, returning a PyTorch tensor.\n",
        "\n",
        "5. **`model(**inputs)`**:  \n",
        "   - Processes the tokenized inputs through the model to obtain output embeddings.\n",
        "\n",
        "6. **`embeddings = outputs.last_hidden_state.mean(dim=1)`**:  \n",
        "   - Averages the last hidden states to create a single vector representing the text.\n",
        "\n",
        "7. **`detach()`**:  \n",
        "   - Detaches the embeddings from the computational graph, transforming them into a NumPy array.\n",
        "\n",
        "This code defines a function to generate text embeddings using the `all-MiniLM-L6-v2` model, enabling applications like similarity search or clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxvn3682CQms",
        "outputId": "6f7691da-80ca-42f7-f398-05466adaaea7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer and model\n",
        "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "def embed_text(text):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "    \n",
        "    # Get model outputs\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        outputs = model(**inputs)\n",
        "        \n",
        "    # Compute embeddings by averaging the last hidden state\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "    return embeddings.cpu().numpy()[0]  # Move to CPU and convert to numpy array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvr6vkjBLVPb"
      },
      "source": [
        "### Loading the Document and Functions to Embed and Store in Pinecone\n",
        "\n",
        "1. **`documents`**:  \n",
        "   - A list containing multiple documents on Artificial Intelligence (AI) and Machine Learning (ML), where each string represents a distinct aspect of these fields.\n",
        "\n",
        "2. **Iterating through Documents**:  \n",
        "   - Loops through each document in the `documents` list, using `i` for the index and `doc` for the document text.\n",
        "\n",
        "3. **`vectorize the text document using the embed_text function`**:  \n",
        "   - Calls the previously defined `embed_text` function to generate an embedding (vector representation) for the current document.\n",
        "\n",
        "4. **`index.upsert(vectors=[(str(i), vector)])`**:  \n",
        "   - Adds or updates the document embedding in the Pinecone index. The `upsert` function requires a list of pairs, with each pair consisting of a unique ID (the string form of index `i`) and its corresponding vector (embedding).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Xa95iuLRCTiH"
      },
      "outputs": [],
      "source": [
        "documents = [\n",
        "    \n",
        "    \"Businesses can leverage AI and ML to automate processes, gain insights from data, improve customer experiences, and drive innovation.\",\n",
        "    \"The history of AI dates back to the 1950s, with pioneers like Alan Turing and John McCarthy laying the groundwork for future developments in the field.\",\n",
        "    \"Turing's famous test measures a machine's ability to exhibit intelligent behavior indistinguishable from that of a human.\",\n",
        "    \"Deep learning, a subset of machine learning, utilizes neural networks with many layers to analyze various factors of data.\",\n",
        "    \"Convolutional Neural Networks (CNNs) are particularly effective in image processing tasks due to their ability to capture spatial hierarchies.\",\n",
        "    \"Recurrent Neural Networks (RNNs) are designed to work with sequential data, making them suitable for tasks like language modeling and speech recognition.\",\n",
        "    \"Transfer learning allows models trained on one task to be adapted to new but related tasks, saving time and resources.\",\n",
        "    \"AI ethics is an emerging field that addresses the moral implications of AI systems, including fairness, accountability, and transparency.\",\n",
        "    \"Explainable AI (XAI) aims to make AI decisions understandable to humans, increasing trust and reliability in AI systems.\",\n",
        "    \"Natural Language Processing (NLP) has advanced significantly, enabling applications such as chatbots, sentiment analysis, and language translation.\",\n",
        "    \"AI-powered recommendation engines analyze user behavior to provide personalized content and suggestions.\",\n",
        "    \"Predictive analytics uses statistical algorithms and machine learning techniques to identify the likelihood of future outcomes based on historical data.\",\n",
        "    \"Big data plays a crucial role in AI and ML, providing the vast amounts of information necessary for training effective models.\",\n",
        "    \"Federated learning is a decentralized approach where models are trained across multiple devices while keeping data localized, enhancing privacy.\",\n",
        "    \"The Internet of Things (IoT) connects devices that can collect and exchange data, enabling smarter AI applications.\",\n",
        "    \"AI is transforming healthcare by improving diagnostic accuracy, personalizing treatment plans, and streamlining administrative tasks.\",\n",
        "    \"In finance, AI algorithms analyze market trends to make trading decisions and detect fraudulent activities.\",\n",
        "    \"AI-driven automation in manufacturing enhances efficiency, reduces costs, and improves product quality through predictive maintenance.\",\n",
        "    \"Self-learning systems adapt and improve their performance over time without human intervention, paving the way for more advanced applications.\",\n",
        "    \"AI has been used in creative fields, generating art, music, and literature, prompting discussions about the nature of creativity.\",\n",
        "    \"The integration of AI and blockchain technology is creating new opportunities for secure data sharing and decentralized applications.\",\n",
        "    \"As AI technologies advance, there is an increasing focus on creating standards and regulations to ensure their ethical use.\",\n",
        "    \"Robotic Process Automation (RPA) automates repetitive tasks, allowing human workers to focus on more complex and strategic activities.\",\n",
        "    \"AI can enhance supply chain management by predicting demand, optimizing inventory levels, and improving logistics efficiency.\",\n",
        "    \"The use of AI in education offers personalized learning experiences, adaptive assessments, and efficient administrative processes.\",\n",
        "    \"AI-powered virtual assistants improve productivity by scheduling appointments, managing emails, and providing real-time information.\",\n",
        "    \"Ethical considerations are critical as AI systems are deployed in sensitive areas like law enforcement and hiring practices.\",\n",
        "    \"Public perception of AI is mixed, with some embracing its potential while others express concerns about job displacement and surveillance.\",\n",
        "    \"AI research continues to evolve, exploring new algorithms, architectures, and applications that could reshape our future.\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca0AUBrgObe1"
      },
      "source": [
        "### Retrieve Document Section\n",
        "\n",
        "1. **`def get_documents(query, top_k=3):`**:  \n",
        "   - Defines a function `retrieve_documents` that takes a `query` string and an optional `top_k` parameter (default is 3) to specify the number of top matches to retrieve.\n",
        "\n",
        "2. **Query Embedding**:  \n",
        "   - Generates an embedding for the query using the `embed_text(query)` function.\n",
        "\n",
        "3. **Dimension Check**:  \n",
        "   - Confirms the length of the `query_embedding` is 384; raises an error if the dimension is incorrect to ensure compatibility with Pinecone embeddings.\n",
        "\n",
        "4. **Convert to List**:  \n",
        "   - Converts the `query_embedding` from a NumPy array to a list, as Pinecone requires the vector in this format.\n",
        "\n",
        "5. **Search Pinecone Index**:  \n",
        "   - Executes the **`index.query`** function with the `query_embedding` and `top_k` to find the most similar vectors (documents) in the index.\n",
        "\n",
        "6. **Retrieve Relevant Documents**:  \n",
        "   - Extracts relevant documents by matching IDs from `results['matches']` with the original `documents` list, using `int(match['id'])`.\n",
        "\n",
        "7. **Return Relevant Documents**:  \n",
        "   - Returns the most pertinent documents based on the search query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "c4EeJS1EDBmG"
      },
      "outputs": [],
      "source": [
        "def retrieve_documents(query, top_k=3):\n",
        "    # Generate embedding for the query\n",
        "    query_embedding = embed_text(query)\n",
        "\n",
        "    # Ensure the query embedding has the correct dimensions\n",
        "    if len(query_embedding) != 384:\n",
        "        raise ValueError(f\"Query embedding has incorrect dimensions: {len(query_embedding)}\")\n",
        "\n",
        "    # Convert the query embedding to a list format\n",
        "    query_embedding_list = query_embedding.tolist()\n",
        "\n",
        "    # Query the index to retrieve the top_k similar documents\n",
        "    results = index.query(vector=query_embedding_list, top_k=top_k)\n",
        "\n",
        "    # Extract relevant documents using the IDs from the results\n",
        "    relevant_docs = [documents[int(match['id'])] for match in results['matches']]\n",
        "    return relevant_docs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjsp-28jPT4I"
      },
      "source": [
        "### Generate Answer Section\n",
        "\n",
        "1. **`def create_response(relevant_documents, search_query):`**:  \n",
        "   - Defines the `generate_answer` function, which takes a list of `relevant_documents` and a query string.\n",
        "\n",
        "2. **`context`**:  \n",
        "   - Combines the relevant documents into a single string named `context` to provide background information for the response.\n",
        "\n",
        "3. **`response = co.create()`**:  \n",
        "   - Calls the Cohere API's `generate` function to create a response with the following parameters:\n",
        "   - **`model='command-xlarge-nightly'`**: Specifies the Cohere model used for text generation.\n",
        "   - **`prompt=f\"Combine the provided context with the question to form your answer: {context}\\n\\nQuestion: {query}\"`**: Merges the context and the query to create the prompt for generating an answer.\n",
        "   - **`max_tokens=100`**: Limits the output to a maximum of 100 tokens.\n",
        "   - **`temperature=0.7`**: Controls the creativity of the response, balancing randomness and coherence.\n",
        "\n",
        "4. **Return Generated Text**:  \n",
        "   - Returns the content of the first generated reply from the Cohere API via `response.generations[0].text`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "afLyhiV3Csny"
      },
      "outputs": [],
      "source": [
        "def generate_answer(relevant_docs, query):\n",
        "    # Create context by concatenating the relevant documents\n",
        "    context = \" \".join(relevant_docs)\n",
        "\n",
        "    # Call Cohere's Generate API to produce an answer\n",
        "    response = co.generate(\n",
        "        model='command-xlarge-nightly',\n",
        "        prompt=f\"Answer the question based on the following context:\\n{context}\\n\\nQuestion: {query}\",\n",
        "        max_tokens=100,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # Return the generated text from the response\n",
        "    return response.generations[0].text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4OvoTlEP8Vn"
      },
      "source": [
        "### Result Generation\n",
        "\n",
        "1. **User Query**:  \n",
        "   - `query = \"Can you explain what Machine Learning is?\"`  \n",
        "   - This line captures the user's question about Machine Learning.\n",
        "\n",
        "2. **Retrieve Documents**:  \n",
        "   - `retrieved_documents = get_documents(query)`  \n",
        "   - Uses the `retrieve_documents` function to fetch the most relevant documents from Pinecone based on the query embedding.\n",
        "\n",
        "3. **Generate Answer**:  \n",
        "   - Calls the `generate_answer` function with `retrieved_documents` and `query`, storing the result in the variable `answer`.\n",
        "\n",
        "4. **Display Query**:  \n",
        "   - `print(\"Search query:\", query)`  \n",
        "   - Outputs the user's original query.\n",
        "\n",
        "5. **Display Retrieved Documents**:  \n",
        "   - Shows the documents retrieved from Pinecone that are relevant to the search.\n",
        "\n",
        "6. **Display Generated Answer**:  \n",
        "   - `print(\"Generated Answer:\", answer)`  \n",
        "   - Outputs the response generated by Cohere's API based on the retrieved documents and the query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEOqOhgoFMBo",
        "outputId": "7d7198c2-444a-496c-ac2a-a929a8652783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: What are key areas of AI?\n",
            "Retrieved Docs: ['Key areas of AI include: Natural Language Processing (NLP): Enables machines to understand and interact with human language.Computer Vision: Allows machines to interpret and process visual information from the world.Robotics: Involves designing and building robots that can perform tasks autonomously.', 'AI and ML are used in various applications, including recommendation systems (e.g., Netflix), virtual assistants (e.g., Siri), autonomous vehicles, and medical diagnosis.', 'Artificial Intelligence refers to the simulation of human intelligence in machines that are programmed to think and learn. AI systems can perform tasks such as speech recognition, decision-making, and problem-solving.']\n",
            "Generated Answer: The answer is Key areas of AI include Natural Language Processing (NLP), Computer Vision, and Robotics.\n"
          ]
        }
      ],
      "source": [
        "# User query\n",
        "query = \"What are the key areas of AI?\"\n",
        "\n",
        "# Retrieve relevant documents based on the query\n",
        "retrieved_docs = retrieve_documents(query)\n",
        "\n",
        "# Generate an answer using the retrieved documents and the query\n",
        "answer = generate_answer(retrieved_docs, query)\n",
        "\n",
        "# Print the results\n",
        "print(\"Query:\", query)\n",
        "print(\"Retrieved Documents:\", retrieved_docs)\n",
        "print(\"Generated Answer:\", answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr3u5X7QFTyC",
        "outputId": "db720977-2d08-480e-98cf-826c76c4ad5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: What are the catagories of Machine Learning models?\n",
            "Retrieved Docs: ['Machine learning models can be categorized into:Supervised Learning: Models are trained on labeled data to predict outcomes.Unsupervised Learning: Models identify patterns and relationships in unlabeled data.Reinforcement Learning: Models learn to make decisions through trial and error to maximize rewards.', 'Machine Learning is a subset of AI that involves training algorithms to learn from and make predictions or decisions based on data.', 'AI and ML are used in various applications, including recommendation systems (e.g., Netflix), virtual assistants (e.g., Siri), autonomous vehicles, and medical diagnosis.']\n",
            "Generated Answer: The answer is Machine Learning models can be categorized into Supervised Learning, Unsupervised Learning, and Reinforcement Learning.\n"
          ]
        }
      ],
      "source": [
        "# User query\n",
        "query = \"What are the categories of Machine Learning models?\"\n",
        "\n",
        "# Retrieve relevant documents based on the query\n",
        "retrieved_docs = retrieve_documents(query)\n",
        "\n",
        "# Generate an answer using the retrieved documents and the query\n",
        "answer = generate_answer(retrieved_docs, query)\n",
        "\n",
        "# Print the results\n",
        "print(\"Query:\", query)\n",
        "print(\"Retrieved Documents:\", retrieved_docs)\n",
        "print(\"Generated Answer:\", answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eH0x1y7SVWY"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
